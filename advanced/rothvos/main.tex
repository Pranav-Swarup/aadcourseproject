\documentclass[11pt, a4paper]{article}

% --- OVERLEAF-FRIENDLY PREAMBLE ---
% Standard font encoding for pdfLaTeX
\usepackage[T1]{fontenc}
% Use the Latin Modern font - a clean, standard LaTeX font
\usepackage{lmodern}
% For setting page margins
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}

% Load amsmath for math notation like O(...)
\usepackage{amsmath}
\usepackage{amsthm}
% Load amssymb for symbols like \mathbb
\usepackage{amssymb}

% For better-looking tables (if you add them later)
\usepackage{booktabs}

% Load hyperref LAST (with few exceptions) and remove explicit driver option
% Let it auto-detect the driver. Use 'breaklinks' instead of 'hyphens'.
\usepackage[colorlinks=true, allcolors=blue, breaklinks]{hyperref}

% that will be printed with the bold label "Lemma".
\newtheorem{lemma}{Lemma}

% (Optional) If you also want Theorems, Corollaries, etc.
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
% --- END PREAMBLE ---

\title{Detailed Analysis of Advanced Bin Packing Algorithms: \\
From Rothvoß (2013) to Hoberg \& Rothvoß (2015)}
\author{Shreyas Ramasubramanian}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
For over three decades, the $OPT + O(\log^2 OPT)$ guarantee of the Karmarkar-Karp algorithm (1982) stood as the benchmark for additive approximations to the 1D Bin Packing Problem. This review provides a detailed analysis of the two sequential breakthroughs that finally surpassed this bound. We examine the 2013 paper by Rothvoß, focusing on its introduction of discrepancy theory (via the Lovett-Meka algorithm) and the novel ``gluing'' technique used to handle problematic ``spiky'' patterns, achieving an $OPT + O(\log OPT \cdot \log \log OPT)$ bound. We then delve into the 2015 paper by Hoberg \& Rothvoß, detailing its refined ``cleaner'' 2-stage packing mechanism (Items $\rightarrow$ Containers $\rightarrow$ Bins), explaining why this structural change simplifies the analysis and secures the final, tight $OPT + O(\log OPT)$ guarantee.
\end{abstract}

\section{Introduction: The Quest for Additive Guarantees}

The one-dimensional Bin Packing Problem (1DBPP) asks for the minimum number of unit-capacity bins required to pack a given set of items with sizes $s_1, \dots, s_n \in (0, 1]$. Its NP-hardness necessitates approximation algorithms. While simple heuristics like First-Fit Decreasing (FFD) offer good \emph{multiplicative} guarantees ($\approx 1.222 \cdot OPT + \text{const}$), a significant line of theoretical research seeks \emph{additive} guarantees of the form $OPT + C$, where $C$ is a small, slowly growing function, ideally independent of the number of items $n$.

For over 30 years, the landmark result was the \textbf{Karmarkar-Karp (KK82) algorithm}, achieving $OPT_f + O(\log^2 OPT_f)$ bins, where $OPT_f$ is the optimal value of the fractional relaxation. This review focuses on the two papers that finally improved this bound.

\section{Preliminaries: The Gilmore-Gomory LP Relaxation}

Both KK82 and the subsequent breakthroughs leverage the \textbf{Gilmore-Gomory Linear Program (LP) relaxation}.
\begin{itemize}
    \item \textbf{Structure:} The LP operates over \emph{patterns}. A pattern $p$ is a multiset of items that fits into a single bin, represented as a vector $p \in \mathbb{Z}_{\ge 0}^n$ where $p_i$ is the count of item $i$, such that $\sum_{i=1}^n p_i s_i \le 1$.
    \item Let $\mathcal{P}$ be the set of all possible valid patterns. The LP formulation is:
    $$ \min \sum_{p \in \mathcal{P}} x_p \quad \text{subject to} \quad \sum_{p \in \mathcal{P}} p_i x_p \ge b_i \quad \forall i=1, \dots, n, \quad x_p \ge 0 \quad \forall p \in \mathcal{P} $$
    Here, $b_i$ is the required number of items of type $i$, and $x_p$ is a variable representing how many times pattern $p$ is used.
    \item \textbf{Challenge:} The number of possible patterns $|\mathcal{P}|$ can be exponential in $n$. However, the LP has only $n$ constraints. Efficient algorithms (like Ellipsoid method variants or Column Generation, referenced in KK82 and Rothvoß's papers) can find an approximate solution $x$ with cost $\le OPT_f + \delta$ in polynomial time (polynomial in $n$, $\sum b_i$, and $1/\delta$). Often, one works with a basic feasible solution, which has at most $n$ non-zero $x_p$ values.
    \item \textbf{The Core Problem:} The LP yields a fractional optimum $OPT_f$. The challenge is to round the fractional vector $x$ (with potentially millions of components $x_p$) into an integer vector $y$ (representing an actual packing) such that the cost $\sum y_p$ is very close to $\sum x_p$, specifically $\sum y_p \le \sum x_p + C$.
\end{itemize}

\section{The 2013 Breakthrough: Rothvoß}

Rothvoß's 2013 paper, ``Approximating Bin Packing within $O(\log OPT \cdot \log \log OPT)$ bins,'' was the first improvement over KK82. It introduced discrepancy theory as a new rounding tool.

\subsection{The New Tool: Discrepancy Theory via Lovett-Meka (LM12)}
Instead of KK82's rounding, Rothvoß employed the \textbf{Constructive Partial Coloring Lemma} (Lovett \& Meka, 2012).
\begin{itemize}
    \item \textbf{Intuition:} Discrepancy theory aims to find a coloring (e.g., $\pm 1$) for elements of a ground set such that for any given subset, the sum of colors is small (the set is ``balanced''). The LM12 algorithm provides a constructive, randomized way to achieve this, generalized to rounding fractional vectors.
    \item \textbf{The LM12 Guarantee (Lemma 1 in Rothvoß 2013):} Given a starting fractional point $x \in [0, 1]^m$ and constraints defined by vectors $v_1, \dots, v_n$, the algorithm finds a new point $y \in [0, 1]^m$ such that:
        \begin{enumerate}
            \item \textbf{Near-Integrality:} At least half of the coordinates $y_j$ are very close to 0 or 1 ($y_j \in [0, \delta] \cup [1-\delta, 1]$).
            \item \textbf{Low Error (Discrepancy):} The change in each constraint value is bounded: $|v_i y - v_i x| \le \lambda_i ||v_i||_2$.
        \end{enumerate}
\end{itemize}

\noindent
\textbf{The Entropy Condition:} The correctness of the rounding depends on choosing error parameters $\lambda_i$ that satisfy the ``entropy condition'' derived from Lovett-Meka. This condition ensures that the randomized walk (Brownian motion) does not hit the error boundaries too frequently before the variables become integral. Mathematically, for a system with $m$ variables, the parameters must satisfy:
\begin{equation}
\sum_{i=1}^n \exp(-\lambda_i^2 / 16) \le \frac{m}{16}
\end{equation}
Rothvoß (2013) applies this lemma iteratively. In each of the $O(\log m)$ iterations, half of the remaining fractional variables are rounded to $0$ or $1$, while the cumulative error on each constraint $i$ is carefully controlled by the $\lambda_i \|A_i\|_2$ term.

\noindent
Here is the formal statement of the lemma:

\begin{lemma}[Constructive Partial Coloring, Lovett-Meka 2012]
\label{lem:lovett-meka}
Let $A \in \mathbb{R}^{n \times m}$ be a matrix, $x \in [0, 1]^m$ be a fractional solution, and $\lambda_1, \dots, \lambda_n > 0$ be parameters satisfying $\sum_{i=1}^n e^{-\lambda_i^2 / 16} \le \frac{m}{16}$.

There exists a polynomial-time randomized algorithm that finds a point $y \in [0, 1]^m$ such that:
\begin{enumerate}
    \item \textbf{(Low Discrepancy):} The error on each constraint is bounded by the $L_2$-norm of the corresponding row:
    $$ |(Ay)_i - (Ax)_i| \le \lambda_i \|A_i\|_2 \quad \forall i=1, \dots, n $$
    \item \textbf{(Near-Integrality):} For a given $\delta > 0$, at least half of the coordinates are nearly integral. For $m' \ge (1 - \delta)m$ coordinates $j \in [m]$, we have:
    $$ y_j \in [0, \delta] \cup [1 - \delta, 1] $$
\end{enumerate}
\end{lemma}

\subsection{The Core Challenge: ``Spiky'' Patterns and the $L_2$-Norm}
The effectiveness of LM12 hinges on the error bound $|v_i y - v_i x| \le \lambda_i ||v_i||_2$. Rothvoß's insight was that this bound is problematic if $||v_i||_2$ is large.
\begin{itemize}
    \item In the bin packing context, the vectors $v_i$ correspond to (sums of) rows of the pattern matrix $A$. The $i$-th row $A_i$ lists how many times item $i$ appears in each pattern.
    \item A large $||A_i||_2 = \sqrt{\sum_p (A_{ip})^2}$ norm occurs if there are patterns $p$ with large entries $A_{ip}$ — i.e., patterns containing many copies of item $i$.
    \item This is the \textbf{``spiky pattern''} problem: a pattern $p$ heavily utilizing a single item type $i$ (large $A_{ip}$) causes a large $L_2$-norm for row $i$, leading to a potentially large rounding error for that item's constraint. This was particularly problematic for very small items where $A_{ip}$ could be large.
\end{itemize}

\subsection{The Novel Technique: ``Gluing''}
To mitigate the large $L_2$-norms caused by spiky patterns, Rothvoß introduced \textbf{``gluing''} as a pre-processing step (detailed in \textbf{Section 5.2 of the 2013 paper}) before rounding.
\begin{itemize}
    \item \textbf{Trigger:} Gluing is applied when a pattern $p$ (with fractional value $x_p = r/q$) uses many copies ($p_i$) of a \emph{small} item $i$, such that the total size $p_i s_i$ exceeds a threshold (e.g., related to $1/\text{polylog}(n)$). Specifically, if $p_i \ge w \cdot q$ for carefully chosen $w, q$.
    \item \textbf{Mechanism (Figure 1b):} It takes $w \cdot q$ copies of item $i$ within pattern $p$ and conceptually ``glues'' them into $q$ copies of a \emph{new, artificial, larger item} $i'$ with size $s_{i'} = w \cdot s_i$. The pattern $p$ is modified to use $q$ copies of $i'$ instead. The fractional value $x_p$ remains $r/q$.
    \item \textbf{Purpose:} This transformation replaces a large entry $A_{ip}$ (for the small item $i$) with smaller entries ($q$) for the new, larger item $i'$. This ``smooths'' the matrix rows associated with small items, reducing their $L_2$-norms and making the LM12 rounding effective.
    \item \textbf{Limitation:} As noted by Hoberg \& Rothvoß (2015), this gluing procedure in the 2013 paper was complex and primarily effective only for items below a size threshold of $1/\text{polylog}(n)$.
\end{itemize}

\subsection{The Full Algorithm (Section 6) and Result}
The Rothvoß (2013) algorithm iterates $O(\log n)$ times. Each iteration involves the following steps, justified by the source's corresponding proofs:
\begin{enumerate}
    \item \textbf{Discretize:} Utilizing \textbf{Lemma 10 of Rothvoß (2013)}, the algorithm ensures $x_p$ values are multiples of some $1/q = 1/\text{polylog}(n)$.
    \item \textbf{Group \& Glue:} By applying the ``gluing'' technique established in \textbf{Lemma 8 (Rothvoß, 2013)}, the instance is made ``well-spread'' for small items (i.e., ensuring $A_{ip}$ is small relative to the total count $A_i x$ for small $i$).
    \item \textbf{Round:} As detailed in \textbf{Theorem 9 (Rothvoß, 2013)}, the algorithm applies the LM12 procedure to the modified matrix/solution, rounding half the remaining fractional variables with controlled error. The error analysis in Theorem 9 carefully bounds the discrepancy based on the properties achieved by gluing.
\end{enumerate}
The complexity arose because the ``Group \& Glue'' step introduced an error term related to the parameters used (specifically, involving $\log(1/\delta)$ where $\delta$ related to $1/\text{polylog}(n)$), contributing an $O(\log \log OPT)$ factor to the error accumulated in each of the $O(\log OPT)$ iterations.
The final guarantee: $OPT_f + O(\log OPT \cdot \log \log OPT)$. The runtime is polynomial, dominated by solving the LP and the LM12 calls.

\section{The 2015 Refinement: Hoberg \& Rothvoß}

The 2015 Hoberg \& Rothvoß paper, ``A Logarithmic Additive Integrality Gap for Bin Packing,'' achieved the tighter $O(\log OPT)$ bound by introducing a fundamentally cleaner structure.

\subsection{The Core Idea: 2-Stage Packing (Section 2)}
Instead of fixing the spiky pattern problem after the fact, they reformulated the problem to avoid it structurally.
\begin{enumerate}
    \item \textbf{Stage 1: Items $\rightarrow$ Containers.} Define a \textbf{container} $C$ as any valid multiset of original items that fits in a bin ($C \in \mathbb{Z}_{\ge 0}^n$ with $\sum s_i C_i \le 1$). Let $s(C)$ be its total size.
    \item \textbf{Stage 2: Containers $\rightarrow$ Bins.} Define a \textbf{pattern} $p$ now as a multiset of \emph{containers} that fits in a bin ($p \in \mathbb{Z}_{\ge 0}^{\mathcal{C}}$ where $\mathcal{C}$ is the set of all containers, such that $\sum_{C \in \mathcal{C}} p_C s(C) \le 1$).
    \item \textbf{Intermediate Variables:} They introduce integer variables $y_C$ representing the number of times container $C$ is ``created'' or used.
    \item \textbf{Packing Graphs (Figure 1):} The process is modeled with two bipartite graphs:
        \begin{itemize}
            \item $G_1(b, y)$: Matches original items (demand $b_i$) to available slots within the chosen containers (supply $y_C \cdot C_i$).
            \item $G_2(x, y)$: Matches the chosen containers (demand $y_C$) to slots within the final fractional patterns (supply $x_p \cdot p_C$).
        \end{itemize}
    \item \textbf{Deficiency:} The objective is implicitly tied to minimizing a ``deficiency'' metric. Formally, for a packing graph $G=(V_l \cup V_r, E)$, the deficiency is defined as the minimum weight of left-nodes that cannot be covered by any valid assignment $a$:
    \begin{equation}
    \text{def}(G) := \min_{a} \sum_{v \in V_l} s(v) \cdot (\text{mult}(v) - a(\delta(v)))
    \end{equation}
    The algorithm's goal is to ensure that rounding fractional values in $x$ increases this deficiency by only a constant amount $O(1)$.
    [cite_start]

[Image of bipartite matching graph deficiency]
 [cite: 777]
\end{enumerate}

\noindent
This 2-stage packing concept is formalized into a new, structured LP relaxation. Let $\mathcal{C}$ be the set of all valid "containers" (multisets of items that fit in a bin), and let $\mathcal{P_C}$ be the set of all valid "patterns" of containers (multisets of containers that fit in a bin).

The Hoberg \& Rothvoß (2015) algorithm finds a fractional solution $(x, y)$ to the following system:

\begin{center}
\textbf{The 2-Stage Packing Formulation}
\end{center}
\begin{align}
    \min \quad & \sum_{p \in \mathcal{P_C}} x_p \label{eq:obj} \\
    \text{s.t.} \quad & \sum_{C \in \mathcal{C}} y_C \cdot C_i \ge b_i & \forall \text{ items } i \label{eq:stage1} \\
              & \sum_{p \in \mathcal{P_C}} x_p \cdot p_C \ge y_C & \forall \text{ containers } C \in \mathcal{C} \label{eq:stage2} \\
              & x_p \ge 0, \quad y_C \ge 0 & \forall p \in \mathcal{P_C}, C \in \mathcal{C} \nonumber
\end{align}

\noindent
Here, $y_C$ represents the (fractional) number of times container $C$ is "created", and $x_p$ is the number of times pattern $p$ is used.
\begin{itemize}
    \item Equation \eqref{eq:stage1} ensures that all original items $b_i$ are packed into containers (corresponds to graph $G_1$).
    \item Equation \eqref{eq:stage2} ensures that all created containers $y_C$ are packed into the final bins (corresponds to graph $G_2$).
\end{itemize}
The key insight is that the rounding algorithm is only applied to Equation \eqref{eq:stage2}, which packs containers. As noted, a pattern $p$ simply cannot contain an arbitrarily large number of copies ($p_C$) of the same container $C$. This structurally avoids the "spiky" $L_2$-norm problem, as the matrix for \eqref{eq:stage2} is inherently "smooth".

\subsection{The Final Algorithm and Bound}
The Hoberg \& Rothvoß (2015) algorithm also iterates $O(\log n)$ times:
\begin{enumerate}
    \item \textbf{Rebuild Containers:} Detailed in \textbf{Section 3 of Hoberg \& Rothvoß (2015)}, this step replaces the complex ``gluing''. It involves sophisticated grouping (\textbf{Lemma 10} of the source) and reassigning/combining containers (\textbf{Lemma 13} of the source) within the existing fractional patterns $x$.
    \par\medskip
    \noindent\emph{The Shadow Matrix:} To bound the rounding error, the authors construct a \textbf{Shadow Incidence Matrix} $\tilde{A}$. Unlike the standard matrix $A$, $\tilde{A}$ retains the history of grouped containers, ensuring that for any container class $\sigma$, the $L_1$ norm of the rows remains sufficiently large ($||\tilde{A}_C||_1 \ge \sigma^{-1/2}$). This structural property allows the Lovett-Meka algorithm to round variables with very low discrepancy, specifically bounding the $L_2$ norm of the error vectors.

    \item \textbf{Round:} As described in \textbf{Section 4}, the algorithm applies the \emph{same} LM12 algorithm (specifically \textbf{Claim 14}) to the fractional pattern vector $x$, using constraints derived from the (rebuilt) container incidence matrix $A$.
\end{enumerate}

\subsection{Why it's Cleaner and Better}
The 2-stage structure provides several advantages leading to the cleaner analysis and tighter bound:
\begin{itemize}
    \item \textbf{Inherent Smoothness:} The objects being packed into the final patterns are now ``containers.'' Since containers must have size $s(C) \le 1$, a pattern $p$ simply cannot contain an arbitrarily large number of copies ($p_C$) of the same container $C$. The large entries in the matrix rows that plagued the 2013 analysis for tiny items are structurally avoided when dealing with containers.
    \item \textbf{Simplified Pre-processing:} The ``Rebuilding Containers'' step (Section 3 of the source) achieves the necessary ``well-spread'' properties more elegantly and robustly than the 2013 ``gluing,'' which had limitations on item size. The 2015 paper notes their procedure works even for items/containers up to size $\Omega(1)$.
    \item \textbf{Error Reduction ($O(1)$ Deficiency):} Because the matrix $A$ (representing container patterns) is inherently better behaved, the LM12 rounding step (Section 4) incurs only a constant $O(1)$ increase in total deficiency per iteration, as proven in \textbf{Lemma 17} of the source paper. The complex error term involving $\log \log OPT$ from the 2013 analysis disappears.
    \item \textbf{Full Spectrum Parameters:} The 2015 paper notes they use the ``full spectrum'' of error parameters $\lambda_I$ in LM12, whereas the 2013 paper used only two types, contributing to the cleaner $O(1)$ error per iteration.
\end{itemize}
The final result: The total additive gap is $O(\log OPT)$ iterations $\times$ $O(1)$ error per iteration, yielding the tight $\mathbf{OPT_f + O(\log OPT)}$ bound.

\section{Conclusion}

The progression from Rothvoß (2013) to Hoberg \& Rothvoß (2015) exemplifies refinement in theoretical algorithm design. Both papers leverage the Gilmore-Gomory LP and the powerful Lovett-Meka discrepancy rounding algorithm. However, the 2013 paper required a complex, somewhat limited ``gluing'' mechanism to force the problem structure into shape for the rounding tool, resulting in an $O(\log OPT \cdot \log \log OPT)$ gap. The 2015 paper achieved the final $O(\log OPT)$ gap through a more fundamental insight: redefining the problem via a 2-stage packing (Items $\rightarrow$ Containers $\rightarrow$ Bins). This elegant structural change inherently created the ``smoothness'' needed for discrepancy rounding, leading to a cleaner algorithm, a simpler analysis, and a tighter, likely optimal, additive bound. Visualizing the contrast between ``gluing'' and ``2-stage packing'' will be key to understanding this significant theoretical advancement.

\vspace{1em} % Adds a bit of vertical space
\noindent % Prevents indentation
Formally, the two breakthrough results are:

\begin{theorem}[Rothvoß 2013]
\label{thm:rothvoss2013}
There exists a polynomial-time algorithm that, given a bin packing instance $I$, finds a packing using at most
$$OPT_f(I) + O(\log(OPT_f(I)) \cdot \log\log(OPT_f(I)))$$
bins, where $OPT_f(I)$ is the optimal value of the Gilmore-Gomory LP relaxation.
\end{theorem}

\begin{theorem}[Hoberg \& Rothvoß 2015]
\label{thm:hoberg-rothvoss2015}
There exists a polynomial-time algorithm that, given a bin packing instance $I$, finds a packing using at most
$$OPT_f(I) + O(\log(OPT_f(I)))$$
bins.
\end{theorem}

\end{document}
